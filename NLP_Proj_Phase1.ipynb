{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Proj_Phase1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiCVdYLEeUZs"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re, pickle, os, sys, requests\n",
        "from tqdm import tqdm\n",
        "import shutil\n",
        "import json\n",
        "from glob import glob\n",
        "from google.colab import output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnI6a2JNKLhc"
      },
      "source": [
        "!pip install scrapy beautifulsoup4\n",
        "from bs4 import BeautifulSoup\n",
        "!rm -rf sample_data/\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHCdF7dMnrop"
      },
      "source": [
        "def clean_path(path):\n",
        "  if os.path.isdir(path):\n",
        "    shutil.rmtree(path, ignore_errors=True)\n",
        "  os.makedirs(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu0CFbjIJBfH"
      },
      "source": [
        "def crawl_data():\n",
        "  seasons = {1:6,2:22,3:23,4:14,5:26,6:24,7:24,8:24,9:23}\n",
        "  dialogs = {}\n",
        "  def addd(c, d):\n",
        "    dialogs[c] = [d]\n",
        "\n",
        "  for season in seasons.keys():\n",
        "    # if not season == 9:\n",
        "    #   continue\n",
        "    for episode in tqdm(range(1,seasons[season]+1)):\n",
        "\n",
        "      if episode < 10:\n",
        "        url = f\"https://www.officequotes.net/no{season}-0{episode}.php\"\n",
        "      else:\n",
        "        url = f\"https://www.officequotes.net/no{season}-{episode}.php\"\n",
        "\n",
        "      r = requests.get(url)\n",
        "      soup = BeautifulSoup(r.content, 'html.parser',)\n",
        "      dialogues = soup.find_all('div', class_='quote')\n",
        "\n",
        "      temp = [[t.split(\":</b>\")[0], re.sub(\"([\\(\\[]).*?([\\)\\]])\", \"\", t.split(\":</b>\")[1].replace('\\xa0','').replace('\\u00e2\\u20ac\\u2122',\"'\").replace('¦',' ').replace('â\\x80',\"'\").replace('\\x99','').strip()).lstrip().strip()] for d in dialogues for t in \\\n",
        "      str(d).replace('<div class=\"quote\">','').replace('</div>','').replace('\\t','').replace(' <b>','').replace('<b>','').strip() \\\n",
        "      .split('<br/>') if \"<u>Deleted Scene\" not in t and len(t.split(\":</b>\")) > 1]\n",
        "\n",
        "      [dialogs[d[0].lower()].append(d[1]) if d[0].lower() in dialogs.keys() else addd(d[0].lower(),d[1]) for d in temp]\n",
        "\n",
        "  return dialogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRnVvJU349ZI"
      },
      "source": [
        "dialogs = crawl_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQfN2mQcRQa"
      },
      "source": [
        "# Saving raw data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OV3nMrDmcb-n"
      },
      "source": [
        "path = os.path.join(\"data\",\"raw\",\"all_characters\")\n",
        "clean_path(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p94LXrMsgqVs"
      },
      "source": [
        "def save_all_characters():\n",
        "  path = os.path.join(\"data\",\"raw\",\"all_characters\",\"all_characters.json\")\n",
        "  with open(path, \"w\") as f:\n",
        "    json.dump(dialogs, f,sort_keys=True, indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-dw6q4XmdUs"
      },
      "source": [
        "def load_all_characters():\n",
        "  path = os.path.join(\"data\",\"raw\",\"all_characters\",\"all_characters.json\")\n",
        "  with open(path, 'r') as f:\n",
        "      dialogs = json.load(f)\n",
        "  return dialogs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvRfdRjK9cf-"
      },
      "source": [
        "save_all_characters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdPV7gjedt-1"
      },
      "source": [
        "dialogs = load_all_characters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXWIrklcdqFd"
      },
      "source": [
        "print(len(sent_tokenize(' '.join(dialogs['dwight']))), len(sent_tokenize(' '.join(dialogs['michael']).lower())))\n",
        "print(len(word_tokenize(' '.join(dialogs['dwight']))), len(word_tokenize(' '.join(dialogs['michael']))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VThaiOhCnVOR"
      },
      "source": [
        "## Save Michael and Dwight dialogues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhZsaSv4oLFB"
      },
      "source": [
        "path = os.path.join(\"data\",\"raw\",\"michael_dwight_dialogues\")\n",
        "clean_path(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uQXbpHqNjtb"
      },
      "source": [
        "characters = ['michael', 'dwight']\n",
        "dialogs = {character: dialogs[character] for character in characters}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hK-t3-SJu-n"
      },
      "source": [
        "print(len(dialogs['michael']),len(dialogs['dwight']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT2U5Pi-gAOF"
      },
      "source": [
        "path = os.path.join(\"data\",\"raw\",\"michael_dwight_dialogues\")\n",
        "for character, dialogues in dialogs.items(): \n",
        "  with open(os.path.join(path, f\"{character}.txt\"), \"w\") as txt_file:\n",
        "    for dialogue in dialogues:\n",
        "      txt_file.write(f\"{dialogue}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68O9n9NktYIl"
      },
      "source": [
        "# Clean up data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgkNwXPVn7Zz"
      },
      "source": [
        "path = os.path.join(\"data\",\"clean\")\n",
        "clean_path(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbeekS5n8E_S"
      },
      "source": [
        "# !pip install transformers\n",
        "!pip install unidecode contractions\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P46tSFrggePx"
      },
      "source": [
        "import nltk\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import spacy\n",
        "import unidecode\n",
        "import contractions\n",
        "nltk.download('all')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "nlp = spacy.load('en')\n",
        "output.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6QoufqRzodj"
      },
      "source": [
        "def save(character, folder, sents):\n",
        "  path = os.path.join(\"data\",\"clean\", folder)\n",
        "  try:\n",
        "    os.makedirs(path)\n",
        "  except:\n",
        "    pass\n",
        "  path = os.path.join(path, f\"{character}.txt\")\n",
        "  with open(path, \"w\") as txt_file:\n",
        "    for sent in sents:\n",
        "      txt_file.write(f\"{sent}\\n\") # works with any number of elements in a line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fah7DtWimb7c"
      },
      "source": [
        "def remove_accented_chars(text):\n",
        "  text = unidecode.unidecode(text)\n",
        "  return text\n",
        "\n",
        "def expand_contractions(text):\n",
        "  text = contractions.fix(text)\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqPJFj2t_jAn"
      },
      "source": [
        "def export_sentences(character):\n",
        "  return sent_tokenize(' '.join(dialogs[character]).lower())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOkL4ea_slTE"
      },
      "source": [
        "def remove_accent_and_expand(character):\n",
        "  sents = export_sentences(character)\n",
        "  sents_expanded = [expand_contractions(remove_accented_chars(sent)) for sent in sents]\n",
        "  save(character, \"cleaned_broken_sentences\", sents_expanded)\n",
        "  sents_lemmatized = []\n",
        "  word_count = 0\n",
        "  for sent in sents_expanded:\n",
        "    # sents_lemmatized.append(' '.join([word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_  for word in nlp(sent)]))\n",
        "    sent_tokenized = word_tokenize(sent)\n",
        "    if len(sent_tokenized) < 5:\n",
        "      continue\n",
        "    word_count += len(sent_tokenized)\n",
        "    sents_lemmatized.append(' '.join([lemmatizer.lemmatize(word) for word in sent_tokenized]))\n",
        "  save(character, \"cleaned_lemmatized_broken_sentences\", sents_lemmatized)\n",
        "  return sents_lemmatized, len(sents_lemmatized), word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKeSdz96rHNS"
      },
      "source": [
        "characters = [\"michael\",\"dwight\"]\n",
        "final_data = []\n",
        "for character in characters:\n",
        "  output, total_sents, word_count = remove_accent_and_expand(character)\n",
        "  final_data.append(output)\n",
        "  print(f\"total number of {character} sentences and words: {total_sents} , {word_count}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzAdqEEu3trO"
      },
      "source": [
        "# Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAf488P08Lc2"
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGVBHFHXSdj4"
      },
      "source": [
        "path = os.path.join('data','clean','cleaned_lemmatized_broken_sentences')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V42VThXY6x4u"
      },
      "source": [
        "data = {}\n",
        "characters = []\n",
        "for file in glob(os.path.join(path,'*.txt')):\n",
        "  name = os.path.basename(file).split('.')[0]\n",
        "  characters.append(name)\n",
        "  with open(file,'r') as f:\n",
        "    data[name] = [sent.strip() for sent in f.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl6AFZbs9faj"
      },
      "source": [
        "def plot(names, numerical_data, title, shift=False, is_large=False):\n",
        "  if is_large:\n",
        "    fig = plt.figure(figsize=(25,25))\n",
        "  else:\n",
        "    fig = plt.figure()\n",
        "  ax = fig.add_axes([0,0,1,1])\n",
        "  if shift:\n",
        "    plt.xticks(rotation=90)\n",
        "    ax.xaxis.set_tick_params(labelsize=10)\n",
        "  ax.bar(names,numerical_data)\n",
        "  plt.title(title)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_stYljb--2E"
      },
      "source": [
        "num = [sum([len(data[char]) for char in characters])]\n",
        "print(num[0])\n",
        "plot([\"sentences\"],num,\"total sentence count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE8paRCQ-QaL"
      },
      "source": [
        "words = sum([len(' '.join(data[char]).split()) for char in characters])\n",
        "distinct_words = sum([ len(list(set(' '.join(data[char]).split()))) for char in characters])\n",
        "num = [ sum([len(data[char]) for char in characters]), words, distinct_words]\n",
        "print(f\"total number of words: {num[0]} and distinct words: {num[1]}\")\n",
        "plot([\"sentences\", \"words\", \"distinct words\"],num,\"total word count\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRDsdu7N7p3K"
      },
      "source": [
        "sentence_count = [len(data[characters[0]]), len(data[characters[1]])]\n",
        "plot(characters,sentence_count,\"sentence count based on each character\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx7FXxKG4s5h"
      },
      "source": [
        "michael_distinct = list(set(' '.join(data[\"michael\"]).split()))\n",
        "dwight_distinct = list(set(' '.join(data[\"dwight\"]).split()))\n",
        "both = 0\n",
        "only_michael = 0\n",
        "only_dwight = 0\n",
        "\n",
        "for word in michael_distinct:\n",
        "  if word in dwight_distinct:\n",
        "    both += 1\n",
        "  else:\n",
        "    only_michael += 1\n",
        "\n",
        "for word in dwight_distinct:\n",
        "  if word not in michael_distinct:\n",
        "    only_dwight += 1\n",
        "\n",
        "print(f\"distinct words in both classes: {both} , first class only: {only_dwight} , second class only: {only_michael}\")\n",
        "\n",
        "plot([\"both\" , \"dwight\" , \"michael\"],[both,only_michael,only_dwight],\"distinct words based on each group\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhGUbhjdBx6W"
      },
      "source": [
        "def word_count(str):\n",
        "    counts = dict()\n",
        "    words = str.split()\n",
        "\n",
        "    for word in words:\n",
        "        if word in counts:\n",
        "            counts[word] += 1\n",
        "        else:\n",
        "            counts[word] = 1\n",
        "\n",
        "    return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6Zv5PIwElMK"
      },
      "source": [
        "michael_distinct_count = word_count(' '.join(data[\"michael\"]))\n",
        "dwight_distinct_count = word_count(' '.join(data[\"dwight\"]))\n",
        "michael_distinct_count = dict(sorted(michael_distinct_count.items(), key=lambda item: item[1], reverse = True))\n",
        "dwight_distinct_count = dict(sorted(dwight_distinct_count.items(), key=lambda item: item[1], reverse = True))\n",
        "\n",
        "dwight_most_repeated = []\n",
        "michael_most_repeated = []\n",
        "for word, count in michael_distinct_count.items():\n",
        "  if word not in dwight_distinct_count.keys():\n",
        "    michael_most_repeated.append((word,count))\n",
        "  if len(michael_most_repeated) >= 10:\n",
        "    break\n",
        "\n",
        "for word, count in dwight_distinct_count.items():\n",
        "  if word not in michael_distinct_count.keys():\n",
        "    dwight_most_repeated.append((word,count))\n",
        "  if len(dwight_most_repeated) >= 10:\n",
        "    break\n",
        "\n",
        "print(dwight_most_repeated)\n",
        "print(michael_most_repeated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C7tcXkyEEa8"
      },
      "source": [
        "plot([x[0] for x in dwight_most_repeated], [x[1] for x in dwight_most_repeated], \"Dwight\",True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzMihXrZW6tj"
      },
      "source": [
        "plot([x[0] for x in michael_most_repeated], [x[1] for x in michael_most_repeated], \"Michael\",True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4upUpZmXC3c"
      },
      "source": [
        "word_frequency = word_count(' '.join(data[\"michael\"]) + ' '.join(data[\"dwight\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbtKJwiJUOe6"
      },
      "source": [
        "word_frequency = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDXQSXevx3QW"
      },
      "source": [
        "_ = [print(x) for x in list(word_frequency.items())[:10]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfP5daMBWQ0L"
      },
      "source": [
        "plot(list(word_frequency.keys())[:180], list(word_frequency.values())[:180], \"Histogram of Word Frequencies\", True, True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI433fNgjgQP"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd\n",
        "vectorizer = TfidfVectorizer(analyzer='word' , stop_words='english')\n",
        "vectors = vectorizer.fit_transform([' '.join(data['michael']), ' '.join(data['dwight'])])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HxButGUtuaBN"
      },
      "source": [
        "a = dict.fromkeys(feature_names, denselist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIsV-WUOua9o"
      },
      "source": [
        "a = dict(sorted(a.items(), key=lambda item: item[1], reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2VDdtjpwTAu"
      },
      "source": [
        "[print(x) for x in list(a.items())[100]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xooNHAF2wUHK"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQRHQ44RzwRN"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "# stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kxpW_8_zLEw"
      },
      "source": [
        "def computeTF(wordDict, bagOfWords):\n",
        "    tfDict = {}\n",
        "    bagOfWordsCount = len(bagOfWords)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count / float(bagOfWordsCount)\n",
        "    return tfDict\n",
        "\n",
        "def computeIDF(documents):\n",
        "    import math\n",
        "    N = len(documents)\n",
        "    \n",
        "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
        "    for document in documents:\n",
        "        for word, val in document.items():\n",
        "            if val > 0:\n",
        "                idfDict[word] += 1\n",
        "    \n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log(N / float(val))\n",
        "    return idfDict\n",
        "  \n",
        "\n",
        "def computeTFIDF(tfBagOfWords, idfs):\n",
        "    tfidf = {}\n",
        "    for word, val in tfBagOfWords.items():\n",
        "        tfidf[word] = val * idfs[word]\n",
        "    return tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCvUozkxz5oY"
      },
      "source": [
        "documentA = ' '.join(data['michael'])\n",
        "documentB = ' '.join(data['dwight'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iUGKB5kzmje"
      },
      "source": [
        "bagOfWordsA = documentA.split(' ')\n",
        "bagOfWordsB = documentB.split(' ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvdl72UV0UMC"
      },
      "source": [
        "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-TxEYKd0V8N"
      },
      "source": [
        "numOfWordsA = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsA:\n",
        "    numOfWordsA[word] += 1\n",
        "numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
        "for word in bagOfWordsB:\n",
        "    numOfWordsB[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1YmgSKY0YH6"
      },
      "source": [
        "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
        "tfB = computeTF(numOfWordsB, bagOfWordsB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFo5UUkG0lTl"
      },
      "source": [
        "idfs = computeIDF([numOfWordsA, numOfWordsB])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi2H9wP50nNT"
      },
      "source": [
        "tfidfA = computeTFIDF(tfA, idfs)\n",
        "tfidfB = computeTFIDF(tfB, idfs)\n",
        "df = pd.DataFrame([tfidfA, tfidfB])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P68F-OWZ0spp"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVGmdGtD0tMO"
      },
      "source": [
        "tfidf_1 = {}\n",
        "tfidf_2 = {}\n",
        "for k in df.keys():\n",
        "  tfidf_1[k] = df[k][0]\n",
        "  tfidf_2[k] = df[k][1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpx3Jl8L004J"
      },
      "source": [
        "tfidf_1 = dict(sorted(tfidf_1.items(), key=lambda item: item[1], reverse=True))\n",
        "tfidf_2 = dict(sorted(tfidf_2.items(), key=lambda item: item[1], reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdzi7Q1l2iN4"
      },
      "source": [
        "_ = [print(item) for item in list(tfidf_1.items())[:10]]\n",
        "plot(list(tfidf_1.keys())[:10], list(tfidf_1.values())[:10], \"TF-IDF of Word Frequencies - Michael\",True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PuBKjwo2q9S"
      },
      "source": [
        "_ = [print(item) for item in list(tfidf_2.items())[:10]]\n",
        "plot(list(tfidf_2.keys())[:10], list(tfidf_2.values())[:10], \"TF-IDF of Word Frequencies - Dwight\",True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USkuPGCscxtS"
      },
      "source": [
        "def compute_RNF(docA, docB):\n",
        "  wc_A = word_count(docA)\n",
        "  wc_B = word_count(docB)\n",
        "  total_A = sum([value for value in wc_A.values()])\n",
        "  total_B = sum([value for value in wc_B.values()])\n",
        "\n",
        "  RNF = {}\n",
        "\n",
        "  for word in wc_A.keys():\n",
        "    if word not in wc_B.keys():\n",
        "      continue\n",
        "    RNF[word] = (wc_A[word]/total_A)/(wc_B[word]/total_B)\n",
        "  \n",
        "  return dict(sorted(RNF.items(), key=lambda item: item[1], reverse=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muRxyx2SiPml"
      },
      "source": [
        "RNF_A = compute_RNF(documentA, documentB)\n",
        "RNF_B = compute_RNF(documentB, documentA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6cZDKPOicWf"
      },
      "source": [
        "_ = [print(item) for item in list(RNF_A.items())[:10]]\n",
        "plot(list(RNF_A.keys())[:10], list(RNF_A.values())[:10], \"RNF - Michael\",True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSNE-SS2ik3P"
      },
      "source": [
        "_ = [print(item) for item in list(RNF_B.items())[:10]]\n",
        "plot(list(RNF_B.keys())[:10], list(RNF_B.values())[:10], \"RNF - Dwight\",True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}